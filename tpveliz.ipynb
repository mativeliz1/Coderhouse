{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests import status_codes\n",
    "\n",
    "from pytrends import exceptions\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "BASE_TRENDS_URL = 'https://trends.google.com/trends'\n",
    "\n",
    "\n",
    "class TrendReq(object):\n",
    "    \"\"\"\n",
    "    Google Trends API\n",
    "    \"\"\"\n",
    "    GET_METHOD = 'get'\n",
    "    POST_METHOD = 'post'\n",
    "    GENERAL_URL = f'{BASE_TRENDS_URL}/api/explore'\n",
    "    INTEREST_OVER_TIME_URL = f'{BASE_TRENDS_URL}/api/widgetdata/multiline'\n",
    "    MULTIRANGE_INTEREST_OVER_TIME_URL = f'{BASE_TRENDS_URL}/api/widgetdata/multirange'\n",
    "    INTEREST_BY_REGION_URL = f'{BASE_TRENDS_URL}/api/widgetdata/comparedgeo'\n",
    "    RELATED_QUERIES_URL = f'{BASE_TRENDS_URL}/api/widgetdata/relatedsearches'\n",
    "    TRENDING_SEARCHES_URL = f'{BASE_TRENDS_URL}/hottrends/visualize/internal/data'\n",
    "    TOP_CHARTS_URL = f'{BASE_TRENDS_URL}/api/topcharts'\n",
    "    SUGGESTIONS_URL = f'{BASE_TRENDS_URL}/api/autocomplete/'\n",
    "    CATEGORIES_URL = f'{BASE_TRENDS_URL}/api/explore/pickers/category'\n",
    "    TODAY_SEARCHES_URL = f'{BASE_TRENDS_URL}/api/dailytrends'\n",
    "    REALTIME_TRENDING_SEARCHES_URL = f'{BASE_TRENDS_URL}/api/realtimetrends'\n",
    "    ERROR_CODES = (500, 502, 504, 429)\n",
    "\n",
    "    def __init__(self, hl='en-US', tz=360, geo='', timeout=(2, 5), proxies='',\n",
    "                 retries=0, backoff_factor=0, requests_args=None):\n",
    "        \"\"\"\n",
    "        Initialize default values for params\n",
    "        \"\"\"\n",
    "        # google rate limit\n",
    "        self.google_rl = 'You have reached your quota limit. Please try again later.'\n",
    "        self.results = None\n",
    "        # set user defined options used globally\n",
    "        self.tz = tz\n",
    "        self.hl = hl\n",
    "        self.geo = geo\n",
    "        self.kw_list = list()\n",
    "        self.timeout = timeout\n",
    "        self.proxies = proxies  # add a proxy option\n",
    "        self.retries = retries\n",
    "        self.backoff_factor = backoff_factor\n",
    "        self.proxy_index = 0\n",
    "        self.requests_args = requests_args or {}\n",
    "        self.cookies = self.GetGoogleCookie()\n",
    "        # intialize widget payloads\n",
    "        self.token_payload = dict()\n",
    "        self.interest_over_time_widget = dict()\n",
    "        self.interest_by_region_widget = dict()\n",
    "        self.related_topics_widget_list = list()\n",
    "        self.related_queries_widget_list = list()\n",
    "\n",
    "        self.headers = {'accept-language': self.hl}\n",
    "        self.headers.update(self.requests_args.pop('headers', {}))\n",
    "        \n",
    "    def GetGoogleCookie(self):\n",
    "        \"\"\"\n",
    "        Gets google cookie (used for each and every proxy; once on init otherwise)\n",
    "        Removes proxy from the list on proxy error\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            if \"proxies\" in self.requests_args:\n",
    "                try:\n",
    "                    return dict(filter(lambda i: i[0] == 'NID', requests.post(\n",
    "                        f'{BASE_TRENDS_URL}/?geo={self.hl[-2:]}',\n",
    "                        timeout=self.timeout,\n",
    "                        **self.requests_args\n",
    "                    ).cookies.items()))\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                if len(self.proxies) > 0:\n",
    "                    proxy = {'https': self.proxies[self.proxy_index]}\n",
    "                else:\n",
    "                    proxy = ''\n",
    "                try:\n",
    "                    return dict(filter(lambda i: i[0] == 'NID', requests.post(\n",
    "                        f'{BASE_TRENDS_URL}/?geo={self.hl[-2:]}',\n",
    "                        timeout=self.timeout,\n",
    "                        proxies=proxy,\n",
    "                        **self.requests_args\n",
    "                    ).cookies.items()))\n",
    "                except requests.exceptions.ProxyError:\n",
    "                    print('Proxy error. Changing IP')\n",
    "                    if len(self.proxies) > 1:\n",
    "                        self.proxies.remove(self.proxies[self.proxy_index])\n",
    "                    else:\n",
    "                        print('No more proxies available. Bye!')\n",
    "                        raise\n",
    "                    continue\n",
    "\n",
    "    def GetNewProxy(self):\n",
    "        \"\"\"\n",
    "        Increment proxy INDEX; zero on overflow\n",
    "        \"\"\"\n",
    "        if self.proxy_index < (len(self.proxies) - 1):\n",
    "            self.proxy_index += 1\n",
    "        else:\n",
    "            self.proxy_index = 0\n",
    "\n",
    "    def _get_data(self, url, method=GET_METHOD, trim_chars=0, **kwargs):\n",
    "        \"\"\"Send a request to Google and return the JSON response as a Python object\n",
    "        :param url: the url to which the request will be sent\n",
    "        :param method: the HTTP method ('get' or 'post')\n",
    "        :param trim_chars: how many characters should be trimmed off the beginning of the content of the response\n",
    "            before this is passed to the JSON parser\n",
    "        :param kwargs: any extra key arguments passed to the request builder (usually query parameters or data)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        s = requests.session()\n",
    "        # Retries mechanism. Activated when one of statements >0 (best used for proxy)\n",
    "        if self.retries > 0 or self.backoff_factor > 0:\n",
    "            retry = Retry(total=self.retries, read=self.retries,\n",
    "                          connect=self.retries,\n",
    "                          backoff_factor=self.backoff_factor,\n",
    "                          status_forcelist=TrendReq.ERROR_CODES,\n",
    "                          allowed_methods=frozenset(['GET', 'POST']))\n",
    "            s.mount('https://', HTTPAdapter(max_retries=retry))\n",
    "\n",
    "        s.headers.update(self.headers)\n",
    "        if len(self.proxies) > 0:\n",
    "            self.cookies = self.GetGoogleCookie()\n",
    "            s.proxies.update({'https': self.proxies[self.proxy_index]})\n",
    "        if method == TrendReq.POST_METHOD:\n",
    "            response = s.post(url, timeout=self.timeout,\n",
    "                              cookies=self.cookies, **kwargs,\n",
    "                              **self.requests_args)  # DO NOT USE retries or backoff_factor here\n",
    "        else:\n",
    "            response = s.get(url, timeout=self.timeout, cookies=self.cookies,\n",
    "                             **kwargs, **self.requests_args)  # DO NOT USE retries or backoff_factor here\n",
    "        # check if the response contains json and throw an exception otherwise\n",
    "        # Google mostly sends 'application/json' in the Content-Type header,\n",
    "        # but occasionally it sends 'application/javascript\n",
    "        # and sometimes even 'text/javascript\n",
    "        if response.status_code == 200 and 'application/json' in \\\n",
    "                response.headers['Content-Type'] or \\\n",
    "                'application/javascript' in response.headers['Content-Type'] or \\\n",
    "                'text/javascript' in response.headers['Content-Type']:\n",
    "            # trim initial characters\n",
    "            # some responses start with garbage characters, like \")]}',\"\n",
    "            # these have to be cleaned before being passed to the json parser\n",
    "            content = response.text[trim_chars:]\n",
    "            # parse json\n",
    "            self.GetNewProxy()\n",
    "            return json.loads(content)\n",
    "        else:\n",
    "            if response.status_code == status_codes.codes.too_many_requests:\n",
    "                raise exceptions.TooManyRequestsError.from_response(response)\n",
    "            raise exceptions.ResponseError.from_response(response)\n",
    "\n",
    "    def build_payload(self, kw_list, cat=0, timeframe='today 5-y', geo='',\n",
    "                      gprop=''):\n",
    "        \"\"\"Create the payload for related queries, interest over time and interest by region\"\"\"\n",
    "        if gprop not in ['', 'images', 'news', 'youtube', 'froogle']:\n",
    "            raise ValueError('gprop must be empty (to indicate web), images, news, youtube, or froogle')\n",
    "        self.kw_list = kw_list\n",
    "        self.geo = geo or self.geo\n",
    "        self.token_payload = {\n",
    "            'hl': self.hl,\n",
    "            'tz': self.tz,\n",
    "            'req': {'comparisonItem': [], 'category': cat, 'property': gprop}\n",
    "        }\n",
    "\n",
    "        # Check if timeframe is a list\n",
    "        if isinstance(timeframe, list):\n",
    "            for index, kw in enumerate(self.kw_list):\n",
    "                keyword_payload = {'keyword': kw, 'time': timeframe[index], 'geo': self.geo}\n",
    "                self.token_payload['req']['comparisonItem'].append(keyword_payload)\n",
    "        else: \n",
    "            # build out json for each keyword with\n",
    "            for kw in self.kw_list:\n",
    "                keyword_payload = {'keyword': kw, 'time': timeframe, 'geo': self.geo}\n",
    "                self.token_payload['req']['comparisonItem'].append(keyword_payload)\n",
    "\n",
    "        # requests will mangle this if it is not a string\n",
    "        self.token_payload['req'] = json.dumps(self.token_payload['req'])\n",
    "        # get tokens\n",
    "        self._tokens()\n",
    "        return\n",
    "\n",
    "    def _tokens(self):\n",
    "        \"\"\"Makes request to Google to get API tokens for interest over time, interest by region and related queries\"\"\"\n",
    "        # make the request and parse the returned json\n",
    "        widget_dicts = self._get_data(\n",
    "            url=TrendReq.GENERAL_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            params=self.token_payload,\n",
    "            trim_chars=4,\n",
    "        )['widgets']\n",
    "        # order of the json matters...\n",
    "        first_region_token = True\n",
    "        # clear self.related_queries_widget_list and self.related_topics_widget_list\n",
    "        # of old keywords'widgets\n",
    "        self.related_queries_widget_list[:] = []\n",
    "        self.related_topics_widget_list[:] = []\n",
    "        # assign requests\n",
    "        for widget in widget_dicts:\n",
    "            if widget['id'] == 'TIMESERIES':\n",
    "                self.interest_over_time_widget = widget\n",
    "            if widget['id'] == 'GEO_MAP' and first_region_token:\n",
    "                self.interest_by_region_widget = widget\n",
    "                first_region_token = False\n",
    "            # response for each term, put into a list\n",
    "            if 'RELATED_TOPICS' in widget['id']:\n",
    "                self.related_topics_widget_list.append(widget)\n",
    "            if 'RELATED_QUERIES' in widget['id']:\n",
    "                self.related_queries_widget_list.append(widget)\n",
    "        return\n",
    "\n",
    "    def interest_over_time(self):\n",
    "        \"\"\"Request data from Google's Interest Over Time section and return a dataframe\"\"\"\n",
    "\n",
    "        over_time_payload = {\n",
    "            # convert to string as requests will mangle\n",
    "            'req': json.dumps(self.interest_over_time_widget['request']),\n",
    "            'token': self.interest_over_time_widget['token'],\n",
    "            'tz': self.tz\n",
    "        }\n",
    "\n",
    "        # make the request and parse the returned json\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.INTEREST_OVER_TIME_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=over_time_payload,\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(req_json['default']['timelineData'])\n",
    "        if (df.empty):\n",
    "            return df\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['time'].astype(dtype='float64'),\n",
    "                                    unit='s')\n",
    "        df = df.set_index(['date']).sort_index()\n",
    "        # split list columns into seperate ones, remove brackets and split on comma\n",
    "        result_df = df['value'].apply(lambda x: pd.Series(\n",
    "            str(x).replace('[', '').replace(']', '').split(',')))\n",
    "        # rename each column with its search term, relying on order that google provides...\n",
    "        for idx, kw in enumerate(self.kw_list):\n",
    "            # there is currently a bug with assigning columns that may be\n",
    "            # parsed as a date in pandas: use explicit insert column method\n",
    "            result_df.insert(len(result_df.columns), kw,\n",
    "                             result_df[idx].astype('int'))\n",
    "            del result_df[idx]\n",
    "\n",
    "        if 'isPartial' in df:\n",
    "            # make other dataframe from isPartial key data\n",
    "            # split list columns into seperate ones, remove brackets and split on comma\n",
    "            df = df.fillna(False)\n",
    "            result_df2 = df['isPartial'].apply(lambda x: pd.Series(\n",
    "                str(x).replace('[', '').replace(']', '').split(',')))\n",
    "            result_df2.columns = ['isPartial']\n",
    "            # Change to a bool type.\n",
    "            result_df2.isPartial = result_df2.isPartial == 'True'\n",
    "            # concatenate the two dataframes\n",
    "            final = pd.concat([result_df, result_df2], axis=1)\n",
    "        else:\n",
    "            final = result_df\n",
    "            final['isPartial'] = False\n",
    "\n",
    "        return final\n",
    "\n",
    "    def multirange_interest_over_time(self):\n",
    "        \"\"\"Request data from Google's Interest Over Time section across different time ranges and return a dataframe\"\"\"\n",
    "\n",
    "        over_time_payload = {\n",
    "            # convert to string as requests will mangle\n",
    "            'req': json.dumps(self.interest_over_time_widget['request']),\n",
    "            'token': self.interest_over_time_widget['token'],\n",
    "            'tz': self.tz\n",
    "        }\n",
    "\n",
    "        # make the request and parse the returned json\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.MULTIRANGE_INTEREST_OVER_TIME_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=over_time_payload,\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(req_json['default']['timelineData'])\n",
    "        if (df.empty):\n",
    "            return df\n",
    "\n",
    "        result_df = pd.json_normalize(df['columnData'])\n",
    "\n",
    "        # Split dictionary columns into seperate ones\n",
    "        for i, column in enumerate(result_df.columns):\n",
    "            result_df[\"[\" + str(i) + \"] \" + str(self.kw_list[i]) + \" date\"] = result_df[i].apply(pd.Series)[\"formattedTime\"]\n",
    "            result_df[\"[\" + str(i) + \"] \" + str(self.kw_list[i]) + \" value\"] = result_df[i].apply(pd.Series)[\"value\"]   \n",
    "            result_df = result_df.drop([i], axis=1)\n",
    "        \n",
    "        # Adds a row with the averages at the top of the dataframe\n",
    "        avg_row = {}\n",
    "        for i, avg in enumerate(req_json['default']['averages']):\n",
    "            avg_row[\"[\" + str(i) + \"] \" + str(self.kw_list[i]) + \" date\"] = \"Average\"\n",
    "            avg_row[\"[\" + str(i) + \"] \" + str(self.kw_list[i]) + \" value\"] = req_json['default']['averages'][i]\n",
    "\n",
    "        result_df.loc[-1] = avg_row\n",
    "        result_df.index = result_df.index + 1\n",
    "        result_df = result_df.sort_index()\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "\n",
    "    def interest_by_region(self, resolution='COUNTRY', inc_low_vol=False,\n",
    "                           inc_geo_code=False):\n",
    "        \"\"\"Request data from Google's Interest by Region section and return a dataframe\"\"\"\n",
    "\n",
    "        # make the request\n",
    "        region_payload = dict()\n",
    "        if self.geo == '':\n",
    "            self.interest_by_region_widget['request'][\n",
    "                'resolution'] = resolution\n",
    "        elif self.geo == 'US' and resolution in ['DMA', 'CITY', 'REGION']:\n",
    "            self.interest_by_region_widget['request'][\n",
    "                'resolution'] = resolution\n",
    "\n",
    "        self.interest_by_region_widget['request'][\n",
    "            'includeLowSearchVolumeGeos'] = inc_low_vol\n",
    "\n",
    "        # convert to string as requests will mangle\n",
    "        region_payload['req'] = json.dumps(\n",
    "            self.interest_by_region_widget['request'])\n",
    "        region_payload['token'] = self.interest_by_region_widget['token']\n",
    "        region_payload['tz'] = self.tz\n",
    "\n",
    "        # parse returned json\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.INTEREST_BY_REGION_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=region_payload,\n",
    "        )\n",
    "        df = pd.DataFrame(req_json['default']['geoMapData'])\n",
    "        if (df.empty):\n",
    "            return df\n",
    "\n",
    "        # rename the column with the search keyword\n",
    "        geo_column = 'geoCode' if 'geoCode' in df.columns else 'coordinates'\n",
    "        columns = ['geoName', geo_column, 'value']\n",
    "        df = df[columns].set_index(['geoName']).sort_index()\n",
    "        # split list columns into separate ones, remove brackets and split on comma\n",
    "        result_df = df['value'].apply(lambda x: pd.Series(\n",
    "            str(x).replace('[', '').replace(']', '').split(',')))\n",
    "        if inc_geo_code:\n",
    "            if geo_column in df.columns:\n",
    "                result_df[geo_column] = df[geo_column]\n",
    "            else:\n",
    "                print('Could not find geo_code column; Skipping')\n",
    "\n",
    "        # rename each column with its search term\n",
    "        for idx, kw in enumerate(self.kw_list):\n",
    "            result_df[kw] = result_df[idx].astype('int')\n",
    "            del result_df[idx]\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def related_topics(self):\n",
    "        \"\"\"Request data from Google's Related Topics section and return a dictionary of dataframes\n",
    "        If no top and/or rising related topics are found, the value for the key \"top\" and/or \"rising\" will be None\n",
    "        \"\"\"\n",
    "\n",
    "        # make the request\n",
    "        related_payload = dict()\n",
    "        result_dict = dict()\n",
    "        for request_json in self.related_topics_widget_list:\n",
    "            # ensure we know which keyword we are looking at rather than relying on order\n",
    "            try:\n",
    "                kw = request_json['request']['restriction'][\n",
    "                    'complexKeywordsRestriction']['keyword'][0]['value']\n",
    "            except KeyError:\n",
    "                kw = ''\n",
    "            # convert to string as requests will mangle\n",
    "            related_payload['req'] = json.dumps(request_json['request'])\n",
    "            related_payload['token'] = request_json['token']\n",
    "            related_payload['tz'] = self.tz\n",
    "\n",
    "            # parse the returned json\n",
    "            req_json = self._get_data(\n",
    "                url=TrendReq.RELATED_QUERIES_URL,\n",
    "                method=TrendReq.GET_METHOD,\n",
    "                trim_chars=5,\n",
    "                params=related_payload,\n",
    "            )\n",
    "\n",
    "            # top topics\n",
    "            try:\n",
    "                top_list = req_json['default']['rankedList'][0]['rankedKeyword']\n",
    "                df_top = pd.json_normalize(top_list, sep='_')\n",
    "            except KeyError:\n",
    "                # in case no top topics are found, the lines above will throw a KeyError\n",
    "                df_top = None\n",
    "\n",
    "            # rising topics\n",
    "            try:\n",
    "                rising_list = req_json['default']['rankedList'][1]['rankedKeyword']\n",
    "                df_rising = pd.json_normalize(rising_list, sep='_')\n",
    "            except KeyError:\n",
    "                # in case no rising topics are found, the lines above will throw a KeyError\n",
    "                df_rising = None\n",
    "\n",
    "            result_dict[kw] = {'rising': df_rising, 'top': df_top}\n",
    "        return result_dict\n",
    "\n",
    "    def related_queries(self):\n",
    "        \"\"\"Request data from Google's Related Queries section and return a dictionary of dataframes\n",
    "        If no top and/or rising related queries are found, the value for the key \"top\" and/or \"rising\" will be None\n",
    "        \"\"\"\n",
    "\n",
    "        # make the request\n",
    "        related_payload = dict()\n",
    "        result_dict = dict()\n",
    "        for request_json in self.related_queries_widget_list:\n",
    "            # ensure we know which keyword we are looking at rather than relying on order\n",
    "            try:\n",
    "                kw = request_json['request']['restriction'][\n",
    "                    'complexKeywordsRestriction']['keyword'][0]['value']\n",
    "            except KeyError:\n",
    "                kw = ''\n",
    "            # convert to string as requests will mangle\n",
    "            related_payload['req'] = json.dumps(request_json['request'])\n",
    "            related_payload['token'] = request_json['token']\n",
    "            related_payload['tz'] = self.tz\n",
    "\n",
    "            # parse the returned json\n",
    "            req_json = self._get_data(\n",
    "                url=TrendReq.RELATED_QUERIES_URL,\n",
    "                method=TrendReq.GET_METHOD,\n",
    "                trim_chars=5,\n",
    "                params=related_payload,\n",
    "            )\n",
    "\n",
    "            # top queries\n",
    "            try:\n",
    "                top_df = pd.DataFrame(\n",
    "                    req_json['default']['rankedList'][0]['rankedKeyword'])\n",
    "                top_df = top_df[['query', 'value']]\n",
    "            except KeyError:\n",
    "                # in case no top queries are found, the lines above will throw a KeyError\n",
    "                top_df = None\n",
    "\n",
    "            # rising queries\n",
    "            try:\n",
    "                rising_df = pd.DataFrame(\n",
    "                    req_json['default']['rankedList'][1]['rankedKeyword'])\n",
    "                rising_df = rising_df[['query', 'value']]\n",
    "            except KeyError:\n",
    "                # in case no rising queries are found, the lines above will throw a KeyError\n",
    "                rising_df = None\n",
    "\n",
    "            result_dict[kw] = {'top': top_df, 'rising': rising_df}\n",
    "        return result_dict\n",
    "\n",
    "    def trending_searches(self, pn='united_states'):\n",
    "        \"\"\"Request data from Google's Hot Searches section and return a dataframe\"\"\"\n",
    "\n",
    "        # make the request\n",
    "        # forms become obsolete due to the new TRENDING_SEARCHES_URL\n",
    "        # forms = {'ajax': 1, 'pn': pn, 'htd': '', 'htv': 'l'}\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.TRENDING_SEARCHES_URL,\n",
    "            method=TrendReq.GET_METHOD\n",
    "        )[pn]\n",
    "        result_df = pd.DataFrame(req_json)\n",
    "        return result_df\n",
    "\n",
    "    def today_searches(self, pn='US'):\n",
    "        \"\"\"Request data from Google Daily Trends section and returns a dataframe\"\"\"\n",
    "        forms = {'ns': 15, 'geo': pn, 'tz': '-180', 'hl': 'en-US'}\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.TODAY_SEARCHES_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=forms,\n",
    "            **self.requests_args\n",
    "        )['default']['trendingSearchesDays'][0]['trendingSearches']\n",
    "        # parse the returned json\n",
    "        result_df = pd.DataFrame(trend['title'] for trend in req_json)\n",
    "        return result_df.iloc[:, -1]\n",
    "\n",
    "    def realtime_trending_searches(self, pn='US', cat='all', count =300):\n",
    "        \"\"\"Request data from Google Realtime Search Trends section and returns a dataframe\"\"\"\n",
    "        # Don't know what some of the params mean here, followed the nodejs library\n",
    "        # https://github.com/pat310/google-trends-api/ 's implemenration\n",
    "\n",
    "\n",
    "        #sort: api accepts only 0 as the value, optional parameter\n",
    "\n",
    "        # ri: number of trending stories IDs returned,\n",
    "        # max value of ri supported is 300, based on emperical evidence\n",
    "\n",
    "        ri_value = 300\n",
    "        if count < ri_value:\n",
    "            ri_value = count\n",
    "\n",
    "        # rs : don't know what is does but it's max value is never more than the ri_value based on emperical evidence\n",
    "        # max value of ri supported is 200, based on emperical evidence\n",
    "        rs_value = 200\n",
    "        if count < rs_value:\n",
    "            rs_value = count-1\n",
    "\n",
    "        forms = {'ns': 15, 'geo': pn, 'tz': '300', 'hl': 'en-US', 'cat': cat, 'fi' : '0', 'fs' : '0', 'ri' : ri_value, 'rs' : rs_value, 'sort' : 0}\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.REALTIME_TRENDING_SEARCHES_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=forms\n",
    "        )['storySummaries']['trendingStories']\n",
    "\n",
    "        # parse the returned json\n",
    "        wanted_keys = [\"entityNames\", \"title\"]\n",
    "\n",
    "        final_json = [{ key: ts[key] for key in ts.keys() if key in wanted_keys} for ts in req_json ]\n",
    "\n",
    "        result_df = pd.DataFrame(final_json)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def top_charts(self, date, hl='en-US', tz=300, geo='GLOBAL'):\n",
    "        \"\"\"Request data from Google's Top Charts section and return a dataframe\"\"\"\n",
    "\n",
    "        try:\n",
    "            date = int(date)\n",
    "        except:\n",
    "            raise ValueError(\n",
    "                'The date must be a year with format YYYY. See https://github.com/GeneralMills/pytrends/issues/355')\n",
    "\n",
    "        # create the payload\n",
    "        chart_payload = {'hl': hl, 'tz': tz, 'date': date, 'geo': geo,\n",
    "                         'isMobile': False}\n",
    "\n",
    "        # make the request and parse the returned json\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.TOP_CHARTS_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=chart_payload\n",
    "        )\n",
    "        try:\n",
    "            df = pd.DataFrame(req_json['topCharts'][0]['listItems'])\n",
    "        except IndexError:\n",
    "            df = None\n",
    "        return df\n",
    "\n",
    "    def suggestions(self, keyword):\n",
    "        \"\"\"Request data from Google's Keyword Suggestion dropdown and return a dictionary\"\"\"\n",
    "\n",
    "        # make the request\n",
    "        kw_param = quote(keyword)\n",
    "        parameters = {'hl': self.hl}\n",
    "\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.SUGGESTIONS_URL + kw_param,\n",
    "            params=parameters,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5\n",
    "        )['default']['topics']\n",
    "        return req_json\n",
    "\n",
    "    def categories(self):\n",
    "        \"\"\"Request available categories data from Google's API and return a dictionary\"\"\"\n",
    "\n",
    "        params = {'hl': self.hl}\n",
    "\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.CATEGORIES_URL,\n",
    "            params=params,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5\n",
    "        )\n",
    "        return req_json\n",
    "\n",
    "    def get_historical_interest(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\n",
    "            \"\"\"This method has been removed for incorrectness. It will be removed completely in v5.\n",
    "If you'd like similar functionality, please try implementing it yourself and consider submitting a pull request to add it to pytrends.\n",
    "          \n",
    "There is discussion at:\n",
    "https://github.com/GeneralMills/pytrends/pull/542\"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrends= TrendReq(hl='es',retries=3, backoff_factor=20)\n",
    "keywords = [\"Clima\", \"Facebook\",'Instagram','Dolar','Futbol']\n",
    "pytrends.build_payload(keywords, cat=0, timeframe='today 5-y', geo='AR', gprop='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clima</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>Instagram</th>\n",
       "      <th>Dolar</th>\n",
       "      <th>Futbol</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>94</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-06-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-06-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>98</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-06-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>94</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>2018-06-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>91</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>2023-04-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>2023-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-05-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>2023-05-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Clima  Facebook  Instagram  Dolar  Futbol      Fecha\n",
       "0       13        94         12     11       4 2018-06-03\n",
       "1       16        93         10     20       6 2018-06-10\n",
       "2       11        98         11     17       9 2018-06-17\n",
       "3       11        94         11     14       8 2018-06-24\n",
       "4       15        91         11     17       6 2018-07-01\n",
       "..     ...       ...        ...    ...     ...        ...\n",
       "255     30        25         13     58      26 2023-04-23\n",
       "256     34        24         14     29      26 2023-04-30\n",
       "257     20        23         14     26      30 2023-05-07\n",
       "258     32        23         14     27      29 2023-05-14\n",
       "259     50        25         16     21      24 2023-05-21\n",
       "\n",
       "[260 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iot=pytrends.interest_over_time()   \n",
    "#Agrego la columna fecha al df para agregar a la tabla y que sea visible, y elimino la columna isPartial que no aportaba nada \n",
    "iot['Fecha']=iot.index\n",
    "iot=iot.reset_index(drop=True)\n",
    "iot=iot.drop(columns=['isPartial'])\n",
    "iot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redshift successfully!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "url=\"data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws\"\n",
    "data_base=\"data-engineer-database\"\n",
    "user=\"matiasveliz14_coderhouse\"\n",
    "with open(\"C:\\pwd_coder.txt\",'r') as f:\n",
    "    pwd= f.read()\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host='data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com',\n",
    "        dbname=data_base,\n",
    "        user=user,\n",
    "        password=pwd,\n",
    "        port='5439'\n",
    "    )\n",
    "    print(\"Connected to Redshift successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Unable to connect to Redshift.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clima                 int32\n",
       "Facebook              int32\n",
       "Instagram             int32\n",
       "Dolar                 int32\n",
       "Futbol                int32\n",
       "Fecha        datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iot.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extras import execute_values\n",
    "\n",
    "def cargar_en_redshift(conn, table_name, dataframe):\n",
    "    dtypes= dataframe.dtypes\n",
    "    cols= list(dtypes.index )\n",
    "    tipos= list(dtypes.values)\n",
    "    type_map = {'int64': 'INT','int32': 'INT','float64': 'FLOAT','object': 'VARCHAR(50)','bool':'BOOLEAN','datetime64[ns]':'DATE'}\n",
    "    sql_dtypes = [type_map[str(dtype)] for dtype in tipos]\n",
    "    # Definir formato SQL VARIABLE TIPO_DATO\n",
    "    column_defs = [f\"{name} {data_type}\" for name, data_type in zip(cols, sql_dtypes)]\n",
    "    # Combine column definitions into the CREATE TABLE statement\n",
    "    table_schema = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            {', '.join(column_defs)}\n",
    "        );\n",
    "        \"\"\"\n",
    "    # Crear la tabla\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(table_schema)\n",
    "    # Generar los valores a insertar\n",
    "    values = [tuple(x) for x in dataframe.to_numpy()]\n",
    "    # Definir el INSERT\n",
    "    insert_sql = f\"INSERT INTO {table_name} ({', '.join(cols)}) VALUES %s\"\n",
    "    # Execute the transaction to insert the data\n",
    "    cur.execute(\"BEGIN\")\n",
    "    execute_values(cur, insert_sql, values)\n",
    "    cur.execute(\"COMMIT\")\n",
    "    print('Proceso terminado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso terminado\n"
     ]
    }
   ],
   "source": [
    "cargar_en_redshift(conn=conn, table_name='interest_over_time', dataframe=iot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
